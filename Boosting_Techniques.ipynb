{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Boosting Techniques"
      ],
      "metadata": {
        "id": "ccOfNIj4piru"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Question 1: What is Boosting in Machine Learning? Explain how it improves weak learners."
      ],
      "metadata": {
        "id": "JPc5HkFJpnAl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer: Boosting is an ensemble learning technique that combines multiple weak learners to create a strong learner. A weak learner is a model that performs slightly better than random guessing.\n",
        "\n",
        "Boosting improves weak learners by:\n",
        "- Sequential Learning: Training models sequentially, with each new model focusing on the errors of the previous ones.\n",
        "- Weighting: Assigning higher weights to data points that were misclassified in previous models, so subsequent models focus more on these difficult cases.\n",
        "- Combining Models: Aggregating predictions from all models to produce a final prediction.\n"
      ],
      "metadata": {
        "id": "vmK5iLrcqCLa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Question 2: What is the difference between AdaBoost and Gradient Boosting in terms of how models are trained?"
      ],
      "metadata": {
        "id": "t_qvQJkGqLxZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer : AdaBoost and Gradient Boosting are both popular boosting algorithms, but they differ in how models are trained:\n",
        "\n",
        "AdaBoost\n",
        "- Focus on Misclassified Points: AdaBoost focuses on the data points that were misclassified in the previous iteration. It increases the weights of these misclassified points so that the next model pays more attention to them.\n",
        "- Weight Adjustment: Adjusts the weights of data points based on whether they were correctly or incorrectly classified.\n",
        "\n",
        "Gradient Boosting\n",
        "- Focus on Residuals: Gradient Boosting focuses on the residuals or errors of the previous model's predictions. It tries to minimize the loss function by adding new models that correct these residuals.\n",
        "- Gradient Descent: Uses gradient descent optimization to minimize the loss function by adding models in the direction that reduces the loss.\n"
      ],
      "metadata": {
        "id": "VpdXFx6GqaPT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Question 3: How does regularization help in XGBoost?"
      ],
      "metadata": {
        "id": "z21EHoARqhaM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer: XGBoost incorporates regularization techniques to prevent overfitting and improve model generalization. Regularization in XGBoost includes:\n",
        "\n",
        "- L1 Regularization (Lasso): Adds a penalty term proportional to the absolute value of the coefficients. This can lead to sparse models.\n",
        "- L2 Regularization (Ridge): Adds a penalty term proportional to the square of the coefficients. This helps to reduce the magnitude of the coefficients.\n",
        "- Gamma: A minimum loss reduction required to make a further partition on a leaf node of the tree. Higher values lead to fewer splits and a simpler model.\n",
        "- Max Depth: Limits the maximum depth of the tree, which controls the complexity of the model.\n"
      ],
      "metadata": {
        "id": "n39fDALvqmIU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Question 4: Why is CatBoost considered efficient for handling categorical data?"
      ],
      "metadata": {
        "id": "dVj59F5equ3t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer:CatBoost is considered efficient for handling categorical data due to its innovative approach to encoding and processing categorical features:\n",
        "\n",
        "Key Features\n",
        "- Native Categorical Support: CatBoost natively supports categorical features, allowing you to directly input categorical data without needing to perform extensive preprocessing like one-hot encoding.\n",
        "- Ordered Target Encoding: CatBoost uses an ordered target encoding approach that efficiently handles categorical features by leveraging the target variable's information to create more informative encodings.\n",
        "- Efficient Handling: CatBoost's algorithm is optimized to handle categorical features during the tree construction process, reducing the need for manual encoding and minimizing potential information loss.\n"
      ],
      "metadata": {
        "id": "HZ8oiLlFqzjH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Question 5: What are some real-world applications where boosting techniques are preferred over bagging methods?"
      ],
      "metadata": {
        "id": "wMf6Go_tq6_n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer: Boosting techniques are often preferred over bagging methods in various real-world applications due to their ability to handle complex data and improve model accuracy. Some examples include:\n",
        "\n",
        "1. Credit Scoring and Risk Assessment\n",
        "- Why Boosting: Boosting algorithms like XGBoost and LightGBM are effective in handling imbalanced datasets and capturing complex relationships between features, making them well-suited for credit scoring and risk assessment tasks.\n",
        "\n",
        "2. Customer Churn Prediction\n",
        "- Why Boosting: Boosting techniques can identify subtle patterns in customer behavior that indicate a high likelihood of churn, allowing businesses to take proactive measures to retain customers.\n",
        "\n",
        "3. Medical Diagnosis and Disease Prediction\n",
        "- Why Boosting: Boosting algorithms can handle high-dimensional data and complex interactions between features, making them suitable for medical diagnosis and disease prediction tasks where accuracy is critical.\n",
        "\n",
        "4. Fraud Detection\n",
        "- Why Boosting: Boosting techniques can effectively identify rare patterns and anomalies in transaction data, making them well-suited for fraud detection tasks where the goal is to detect unusual behavior.\n",
        "\n",
        "5. Recommendation Systems\n",
        "- Why Boosting: Boosting algorithms can learn complex relationships between user behavior and item features, allowing for more accurate and personalized recommendations.\n"
      ],
      "metadata": {
        "id": "6rWhNetRrAd4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 6: Write a Python program to:\n",
        "\n",
        "●\tTrain an AdaBoost Classifier on the Breast Cancer dataset\n",
        "\n",
        "●\tPrint the model accuracy\n",
        "\n",
        "(Include your Python code and output in the code box below.)\n",
        "\n"
      ],
      "metadata": {
        "id": "63li9eZkrMVw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "def main():\n",
        "    # Load the Breast Cancer dataset\n",
        "    data = load_breast_cancer()\n",
        "    X = data.data\n",
        "    y = data.target\n",
        "\n",
        "    # Split data into train/test sets\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Train an AdaBoost Classifier\n",
        "    model = AdaBoostClassifier(n_estimators=100, random_state=42)\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # Predict and calculate accuracy\n",
        "    y_pred = model.predict(X_test)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "# Print model accuracy\n",
        "    print(f\"**AdaBoost Classifier Accuracy**: {accuracy:.3f}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kUwtGtq_rb9j",
        "outputId": "471efffe-1239-418f-de24-ef1b48f1810f"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "**AdaBoost Classifier Accuracy**: 0.974\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "# Question 7: Write a Python program to:\n",
        "\n",
        "●\tTrain a Gradient Boosting Regressor on the California Housing dataset\n",
        "\n",
        "●\tEvaluate performance using R-squared score\n",
        "\n",
        "(Include your Python code and output in the code box below.)\n",
        "\n"
      ],
      "metadata": {
        "id": "HRBQYzeyrkB8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from sklearn.metrics import r2_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Load the California Housing dataset\n",
        "cal_housing = fetch_california_housing()\n",
        "X = cal_housing.data\n",
        "y = cal_housing.target\n",
        "\n",
        "# Split data into train/test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize the dataset\n",
        "sc = StandardScaler()\n",
        "X_train_std = sc.fit_transform(X_train)\n",
        "X_test_std = sc.transform(X_test)\n",
        "\n",
        "# Train a Gradient Boosting Regressor\n",
        "gbr_model = GradientBoostingRegressor(n_estimators=1000, max_depth=3,\n",
        "learning_rate=0.01, loss='squared_error')\n",
        "gbr_model.fit(X_train_std, y_train)\n",
        "\n",
        "# Predict and calculate R-squared score\n",
        "y_pred = gbr_model.predict(X_test_std)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "# Print R-squared score\n",
        "print(f\"R-squared Score: {r2:.3f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_c5_s3Klr5DO",
        "outputId": "8a65ec46-7b0e-47e3-d241-e6efb7e1e2fe"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "R-squared Score: 0.775\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Question 8: Write a Python program to:\n",
        "\n",
        "●\tTrain an XGBoost Classifier on the Breast Cancer dataset\n",
        "\n",
        "●\tTune the learning rate using GridSearchCV\n",
        "\n",
        "●\tPrint the best parameters and accuracy\n",
        "\n",
        "\n",
        "(Include your Python code and output in the code box below.)\n"
      ],
      "metadata": {
        "id": "K4A4poXCsDmH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define hyperparameter grid\n",
        "param_grid = {\n",
        "    'learning_rate': [0.01, 0.1, 0.3],\n",
        "    'max_depth': [3, 5, 7],\n",
        "    'n_estimators': [50, 100, 200],\n",
        "'subsample': [0.6, 0.8, 1.0],\n",
        "    'colsample_bytree': [0.6, 0.8, 1.0]\n",
        "}\n",
        "\n",
        "# Create XGBoost classifier\n",
        "xgb = XGBClassifier(objective='binary:logistic', random_state=42)\n",
        "\n",
        "# Perform grid search\n",
        "grid_search = GridSearchCV(estimator=xgb, param_grid=param_grid, cv=3, n_jobs=-1, verbose=2)\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Print best parameters\n",
        "print(f\"Best Parameters: {grid_search.best_params_}\")\n",
        "\n",
        "# Train model with best parameters and evaluate accuracy\n",
        "best_model = grid_search.best_estimator_\n",
        "y_pred = best_model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy:.3f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UZsJGuQ6sKkx",
        "outputId": "7b575ee3-ad45-4dc2-e381-a608d69db34a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 3 folds for each of 243 candidates, totalling 729 fits\n",
            "Best Parameters: {'colsample_bytree': 1.0, 'learning_rate': 0.3, 'max_depth': 3, 'n_estimators': 200, 'subsample': 0.6}\n",
            "Accuracy: 0.974\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 9: Write a Python program to:\n",
        "\n",
        "●\tTrain a CatBoost Classifier\n",
        "\n",
        "Plot the confusion matrix using seaborn\n",
        " (Include your Python code and output in the code box below.)\n"
      ],
      "metadata": {
        "id": "LmHLX8W5ssWb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "# Import necessary libraries\n",
        "from catboost import CatBoostClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "import numpy as np\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split data into train/test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train a CatBoost Classifier\n",
        "model = CatBoostClassifier(iterations=100, random_state=42, verbose=False)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict and calculate confusion matrix\n",
        "y_pred = model.predict(X_test)\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Plot confusion matrix using seaborn\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, cmap='Blues')\n",
        "plt.xlabel('Predicted Labels')\n",
        "plt.ylabel('True Labels')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "3afMzwMtv0cE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Question 10: You're working for a FinTech company trying to predict loan default using customer demographics and transaction behavior.\n",
        "The dataset is imbalanced, contains missing values, and has both numeric and\n",
        "categorical features.\n",
        "Describe your step-by-step data science pipeline using boosting techniques:\n",
        "● Data preprocessing & handling missing/categorical values\n",
        "● Choice between AdaBoost, XGBoost, or CatBoost\n",
        "● Hyperparameter tuning strategy\n",
        "● Evaluation metrics you'd choose and why\n",
        "● How the business would benefit from your model\n",
        "(Include your Python code and output in the code box below.)\n"
      ],
      "metadata": {
        "id": "ehUmbKAVuGNv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " Answer : Data Science Pipeline for Predicting Loan Default\n",
        "Step 1: Data Preprocessing & Handling Missing/Categorical Values\n",
        "- Handling Missing Values: Use imputation techniques like mean/median for numeric features and mode for categorical features. For more complex cases, consider using K-Nearest Neighbors (KNN) imputation or model-based imputation.\n",
        "- Encoding Categorical Variables: Use techniques like one-hot encoding, label encoding, or target encoding depending on the nature of the categorical variables and the model used.\n",
        "- Scaling Numeric Features: Apply standardization or normalization to numeric features to ensure they are on a similar scale, which can improve model performance.\n",
        "\n",
        "Step 2: Choice Between AdaBoost, XGBoost, or CatBoost\n",
        "- AdaBoost: Effective for simple models and can handle imbalanced datasets by adjusting weights. However, it might not perform as well as other boosting algorithms on complex datasets.\n",
        "- XGBoost: Known for its performance and flexibility. It handles missing values natively and is highly customizable, making it suitable for complex datasets.\n",
        "- CatBoost: Handles categorical features natively and is effective for datasets with many categorical variables. It also provides good performance and is relatively easy to tune.\n",
        "\n",
        "Given the dataset's characteristics (imbalanced, missing values, both numeric and categorical features), XGBoost or CatBoost would be strong candidates due to their ability to handle these aspects effectively.\n",
        "\n",
        "Step 3: Hyperparameter Tuning Strategy\n",
        "- Grid Search: Use GridSearchCV to systematically search through a predefined set of hyperparameters. This is computationally intensive but thorough.\n",
        "- Random Search: Use RandomizedSearchCV for a more efficient search over a distribution of hyperparameters. This can often find good parameters with less computation.\n",
        "- Bayesian Optimization: Use libraries like Optuna or Hyperopt for Bayesian optimization, which can be more efficient than grid or random search by intelligently exploring the parameter space.\n",
        "\n",
        "Step 4: Evaluation Metrics\n",
        "- Precision: Important to understand how many of the predicted defaults are actually defaults.\n",
        "- Recall: Crucial to capture as many actual defaults as possible to minimize risk.\n",
        "- F1-Score: Balances precision and recall, useful when the dataset is imbalanced.\n",
        "- AUC-ROC: Provides a comprehensive view of the model's performance across different thresholds.\n",
        "\n",
        "Given the imbalanced nature of the dataset and the business context, F1-Score and AUC-ROC would be particularly relevant metrics.\n",
        "\n",
        "Step 5: How the Business Would Benefit from the Model\n",
        "- Risk Management: By accurately predicting loan defaults, the business can make informed decisions about loan approvals, interest rates, and credit limits, thereby minimizing risk.\n",
        "- Customer Targeting: The model can help in identifying high-risk customers early, allowing for proactive measures like offering financial counseling or adjusting loan terms.\n",
        "- Increased Profitability: By reducing the number of defaults, the business can increase its profitability and maintain a healthy loan portfolio.\n",
        "\n"
      ],
      "metadata": {
        "id": "h1m8TZSYuO4g"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E2_M2VueCE",
        "outputId": "8c3f7226-cf22-42e7-e57e-5d8bbd743ec4"
      },
      "source": [
        "from xgboost import XGBRegressor # Changed from XGBClassifier\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import mean_squared_error, r2_score # Changed evaluation metrics\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import pandas as pd\n",
        "from sklearn.datasets import fetch_california_housing # Added import for dataset\n",
        "\n",
        "# Load and preprocess data\n",
        "cal_housing = fetch_california_housing() # Load the dataset\n",
        "X = cal_housing.data\n",
        "y = cal_housing.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Define model and hyperparameter grid\n",
        "model = XGBRegressor(random_state=42) # Changed to XGBRegressor and removed objective\n",
        "param_grid = {\n",
        " 'max_depth': [3, 5, 7],\n",
        " 'learning_rate': [0.01, 0.1, 0.3],\n",
        " 'n_estimators': [50, 100, 200],\n",
        " 'subsample': [0.8, 1.0],\n",
        " 'colsample_bytree': [0.8, 1.0]\n",
        "}\n",
        "\n",
        "# Perform grid search\n",
        "grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=3, n_jobs=-1, verbose=2, scoring='neg_mean_squared_error') # Changed scoring\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate model\n",
        "best_model = grid_search.best_estimator_\n",
        "y_pred = best_model.predict(X_test)\n",
        "\n",
        "mse = mean_squared_error(y_test, y_pred) # Calculate MSE\n",
        "r2 = r2_score(y_test, y_pred) # Calculate R-squared\n",
        "\n",
        "print(f'Best Parameters: {grid_search.best_params_}')\n",
        "print(f'Mean Squared Error: {mse:.3f}') # Print MSE\n",
        "print(f'R-squared: {r2:.3f}') # Print R-squared"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 3 folds for each of 108 candidates, totalling 324 fits\n",
            "Best Parameters: {'colsample_bytree': 0.8, 'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 200, 'subsample': 1.0}\n",
            "Mean Squared Error: 0.200\n",
            "R-squared: 0.848\n"
          ]
        }
      ]
    }
  ]
}